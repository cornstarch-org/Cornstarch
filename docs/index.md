---
description: "A distributed multimodal LLM modeling framework"
hide:
  - toc
  - navigation
---
<div align="center">
<img src="assets/images/cornstarch.svg">
<h1><strong>Cornstarch</strong></h1>
<h2>A Distributed multimodal LLM Modeling Framework</h2>
</div>
---

Cornstarch is a distributed multimodal LLM modeling framework that allows you to create a multimodal LLM as you want, and to train, or serve it.

!!! note

    As of January 2025, Cornstarch only provides training multimodal LLMs and does not support serving.

# Documentation Organization
- [Getting Started](getting_started/installation.md): Instructions on installation and setup.
- [Creating a multimodal LLM](using_cornstarch/creating_mllm.md): How to create a multimodal LLM from unimodal models.
- [Preprocessing multimodal inputs](using_cornstarch/preprocessing_inputs.md): How to preprocess muiltimodal inputs to run a multimodal LLM.
- [Training a multimodal LLM](using_cornstarch/training_mllm.md): How to train a multimodal LLM using Cornstarch.
- [Multimodal LLM Parallelization](using_cornstarch/parallelization.md): How to parallelize a multimodal LLM training.
- [Source Code Reference](references): Auto-generated source code reference.

# Research Works
Below is a list of works powered by Cornstarch.

- [Oobleck](https://github.com/SymbioticLab/Oobleck): resilient distributed training using pipeline template

## Contact
Insu Jang (insujang@umich.edu)